{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXasF9T9zMxAV4qATKSg4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firaolkiya/-Building-an-Amharic-E-commerce-Data-Extractor/blob/main/notebooks/eda_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "TOcFxD1NYx-C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Labeled data set**"
      ],
      "metadata": {
        "id": "wqmxonDyhONL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/labeled_telegram_product_price_location.txt', 'r') as f:\n",
        "    labeled_data = f.read()\n",
        "\n",
        "messages = labeled_data.strip().split('\\n\\n')\n",
        "data = []\n",
        "for message in messages:\n",
        "    lines = message.split('\\n')\n",
        "    for line in lines:\n",
        "        if line: # Handle potential empty lines within a message block\n",
        "            parts = line.split()\n",
        "            if len(parts) == 2:\n",
        "                data.append(parts)\n",
        "\n",
        "labeled_df = pd.DataFrame(data, columns=['Token', 'Label'])\n",
        "\n",
        "display(labeled_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jeLEOFBCZ2Dr",
        "outputId": "d48d6a48-14bf-486d-e65b-9827f8e5435b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Token      Label\n",
              "0      3pcs  B-PRODUCT\n",
              "1   silicon  I-PRODUCT\n",
              "2     brush  I-PRODUCT\n",
              "3  spatulas  I-PRODUCT\n",
              "4       እስከ          O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce648283-fec8-4c3b-9a2c-756deee40940\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3pcs</td>\n",
              "      <td>B-PRODUCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>silicon</td>\n",
              "      <td>I-PRODUCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brush</td>\n",
              "      <td>I-PRODUCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spatulas</td>\n",
              "      <td>I-PRODUCT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>እስከ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce648283-fec8-4c3b-9a2c-756deee40940')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ce648283-fec8-4c3b-9a2c-756deee40940 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ce648283-fec8-4c3b-9a2c-756deee40940');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4a6d5a95-1202-4eb0-aaac-1420827e687d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a6d5a95-1202-4eb0-aaac-1420827e687d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4a6d5a95-1202-4eb0-aaac-1420827e687d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(labeled_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"silicon\",\n          \"\\u12a5\\u1235\\u12a8\",\n          \"brush\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"B-PRODUCT\",\n          \"I-PRODUCT\",\n          \"O\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install requirements**"
      ],
      "metadata": {
        "id": "TKVawoORhIsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "model_name = \"rasyosef/bert-tiny-amharic\"\n",
        "\n",
        "label_list = labeled_df['Label'].unique().tolist()\n",
        "\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_name}\")\n",
        "print(f\"Model loaded with {model.config.num_labels} labels: {model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qxa4pHyg5m8",
        "outputId": "112e5521-0d9a-462c-837f-86fe11edc5e0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded: rasyosef/bert-tiny-amharic\n",
            "Model loaded with 5 labels: rasyosef/bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX2E2jOAido7",
        "outputId": "15261c13-8875-4136-faf3-7c7534e2ce3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(174645, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_inputs = []\n",
        "labels = []\n",
        "\n",
        "current_tokens = []\n",
        "current_labels = []\n",
        "\n",
        "for index, row in labeled_df.iterrows():\n",
        "    token = row['Token']\n",
        "    label = row['Label']\n",
        "\n",
        "    if pd.isna(token):\n",
        "        continue\n",
        "\n",
        "    if not token.strip():\n",
        "        continue\n",
        "\n",
        "    word_tokens = tokenizer.tokenize(token)\n",
        "\n",
        "    if word_tokens:\n",
        "        # Assign the original label to the first subword token\n",
        "        current_tokens.extend(word_tokens)\n",
        "        current_labels.append(label)\n",
        "        # Assign a special value (-100) to subsequent subword tokens\n",
        "        current_labels.extend([-100] * (len(word_tokens) - 1))\n",
        "    else:\n",
        "        # If tokenization results in no tokens (e.g., empty string), skip\n",
        "        continue\n",
        "\n",
        "\n",
        "tokenized_inputs.extend(current_tokens)\n",
        "labels.extend(current_labels)\n",
        "\n",
        "\n",
        "print(\"Example of tokenized inputs and aligned labels:\")\n",
        "for i in range(min(20, len(tokenized_inputs))): # Print first 20 for example\n",
        "    print(f\"Token: {tokenized_inputs[i]}, Label: {labels[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3I1v1WUitxB",
        "outputId": "db929cf8-ce10-4394-c208-ec22494e7dae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of tokenized inputs and aligned labels:\n",
            "Token: 3, Label: B-PRODUCT\n",
            "Token: ##p, Label: -100\n",
            "Token: ##c, Label: -100\n",
            "Token: ##s, Label: -100\n",
            "Token: s, Label: I-PRODUCT\n",
            "Token: ##il, Label: -100\n",
            "Token: ##ic, Label: -100\n",
            "Token: ##on, Label: -100\n",
            "Token: b, Label: I-PRODUCT\n",
            "Token: ##r, Label: -100\n",
            "Token: ##us, Label: -100\n",
            "Token: ##h, Label: -100\n",
            "Token: sp, Label: I-PRODUCT\n",
            "Token: ##at, Label: -100\n",
            "Token: ##ul, Label: -100\n",
            "Token: ##as, Label: -100\n",
            "Token: እስከ, Label: O\n",
            "Token: 26, Label: O\n",
            "Token: ##0, Label: -100\n",
            "Token: ##°, Label: -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up training argumentsand evaluation strategy.**"
      ],
      "metadata": {
        "id": "2tbW414jjrNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        ")\n",
        "\n",
        "print(\"Training arguments set up:\")\n",
        "print(training_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtBqriCgjNNs",
        "outputId": "c50fb3b5-46c5-4efa-c3dc-5502b242c200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments set up:\n",
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=IntervalStrategy.NO,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./logs,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.01,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tune NER Model **"
      ],
      "metadata": {
        "id": "8dZyD_8roXY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "label_list = labeled_df['Label'].unique().tolist()\n",
        "\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n",
        "\n",
        "label_ids = [label_map[label] if label != -100 and label in label_map else -100 for label in labels]\n",
        "\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized_inputs)\n",
        "\n",
        "\n",
        "max_length = max(len(seq) for seq in [input_ids])\n",
        "max_length = 128\n",
        "\n",
        "tokenized_datasets = []\n",
        "label_datasets = []\n",
        "\n",
        "current_input_ids = []\n",
        "current_label_ids = []\n",
        "\n",
        "messages = labeled_data.strip().split('\\n\\n')\n",
        "\n",
        "for message in messages:\n",
        "    tokens = []\n",
        "    labels_str = []\n",
        "    for line in message.split('\\n'):\n",
        "        if line.strip():\n",
        "            parts = line.split()\n",
        "            if len(parts) == 2:\n",
        "                tokens.append(parts[0])\n",
        "                labels_str.append(parts[1])\n",
        "\n",
        "    message_input_ids = []\n",
        "    message_label_ids = []\n",
        "    for token, label in zip(tokens, labels_str):\n",
        "        word_tokens = tokenizer.tokenize(token)\n",
        "        if word_tokens:\n",
        "            message_input_ids.extend(tokenizer.convert_tokens_to_ids(word_tokens))\n",
        "            message_label_ids.append(label_map[label] if label in label_map else -100)\n",
        "            message_label_ids.extend([-100] * (len(word_tokens) - 1))\n",
        "\n",
        "    message_input_ids = message_input_ids[:max_length] + [tokenizer.pad_token_id] * (max_length - len(message_input_ids))\n",
        "    message_label_ids = message_label_ids[:max_length] + [-100] * (max_length - len(message_label_ids))\n",
        "\n",
        "    tokenized_datasets.append(message_input_ids)\n",
        "    label_datasets.append(message_label_ids)\n",
        "\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, input_ids, label_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_ids = label_ids\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx],\n",
        "                'attention_mask': [1 if id != tokenizer.pad_token_id else 0 for id in self.input_ids[idx]],\n",
        "                'labels': self.label_ids[idx]}\n",
        "\n",
        "dataset = NERDataset(tokenized_datasets, label_datasets)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "try:\n",
        "  print(\"Starting training...\")\n",
        "  trainer.train()\n",
        "  print(\"Training finished.\")\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YIpg56Wzjz8r",
        "outputId": "3c40a913-49a4-47b3-e80c-08ce62ac70f5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 02:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.508300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.496900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.449700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.352200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.305300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.265700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.212600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.101800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.980600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.930400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.870600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.806000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.773300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.718900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.685600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.645400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.593800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.528300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.493400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.512800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.408400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.405400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.382700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.262500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.281500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.237400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.216300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.191400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.204400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.190500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.186200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.157200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.153200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.156200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.144300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.136900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.143600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.139100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.139600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.155000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the fine-tuned model on the validation set to check performance**"
      ],
      "metadata": {
        "id": "RboEIghao_f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating on the training dataset (for demonstration purposes)...\")\n",
        "evaluation_results = trainer.evaluate(eval_dataset=dataset)\n",
        "print(\"Evaluation results (on training set):\")\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "FMrjEEp1kqZY",
        "outputId": "406a466d-2b9a-4e58-cc17-8a80c3acdbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on the training dataset (for demonstration purposes)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results (on training set):\n",
            "{'eval_loss': 0.11596839874982834, 'eval_runtime': 16.8, 'eval_samples_per_second': 188.453, 'eval_steps_per_second': 2.976, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = './fine_tuned_ner_model'\n",
        "trainer.save_model(output_dir)\n",
        "\n",
        "print(f\"Model saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-kBvOgwo0r8",
        "outputId": "a8ac1867-0703-44e5-d5c0-14b5aa13db67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./fine_tuned_ner_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohrzuIqepNny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b635325c"
      },
      "source": [
        "# Task\n",
        "Fine-tune multiple pre-trained models (XLM-Roberta, DistilBERT, mBERT, and bert-tiny-amharic) for Named Entity Recognition on the provided Amharic dataset, evaluate their performance, and save the fine-tuned models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "906b6b19"
      },
      "source": [
        "## Define a list of models to fine-tune\n",
        "\n",
        "### Subtask:\n",
        "Create a list containing the names of the pre-trained models you want to fine-tune (e.g., \"xlm-roberta-base\", \"distilbert-base-multilingual-cased\", \"bert-base-multilingual-cased\", \"Davlan/bert-tiny-amharic\").\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afd5548"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of model names as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e28402f",
        "outputId": "97072fa2-be39-439f-c539-16d843c1f6ac"
      },
      "source": [
        "model_names = [\n",
        "    \"xlm-roberta-base\",\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    \"rasyosef/bert-tiny-amharic\"\n",
        "]\n",
        "\n",
        "print(model_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['xlm-roberta-base', 'distilbert-base-multilingual-cased', 'bert-base-multilingual-cased', 'rasyosef/bert-tiny-amharic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d69d282b"
      },
      "source": [
        "## Iterate through the list of models\n",
        "\n",
        "### Subtask:\n",
        "Loop through each model name in your list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2553201"
      },
      "source": [
        "**Reasoning**:\n",
        "Start a for loop to iterate through the list of model names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90923742",
        "outputId": "13402edd-d49e-4a49-c41a-d42e48660d15"
      },
      "source": [
        "for model_name in model_names:\n",
        "    print(f\"Processing model: {model_name}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: xlm-roberta-base\n",
            "Processing model: distilbert-base-multilingual-cased\n",
            "Processing model: bert-base-multilingual-cased\n",
            "Processing model: rasyosef/bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034a7e6b"
      },
      "source": [
        "## Load the tokenizer and model\n",
        "\n",
        "### Subtask:\n",
        "Inside the loop, load the tokenizer and the `AutoModelForTokenClassification` for the current model name, ensuring `num_labels` is set correctly based on your dataset's `label_list`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7db90676"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the tokenizer and model for the current model name inside the loop, ensuring the correct number of labels and label mappings are set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29b7143b",
        "outputId": "107c746c-7a91-4120-f7ee-53430555ed5f"
      },
      "source": [
        "try:\n",
        "  for model_name in model_names:\n",
        "      print(f\"Processing model: {model_name}\")\n",
        "\n",
        "      # Load tokenizer\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      print(f\"Tokenizer loaded: {model_name}\")\n",
        "\n",
        "      # Load model\n",
        "      model = AutoModelForTokenClassification.from_pretrained(\n",
        "          model_name,\n",
        "          num_labels=len(label_list),  # Explicitly set the number of labels\n",
        "          id2label=id2label,          # Add mapping for saving with the model\n",
        "          label2id=label2id           # Add mapping for saving with the model\n",
        "      )\n",
        "      print(f\"Model loaded with {model.config.num_labels} labels: {model_name}\")\n",
        "\n",
        "      # The rest of the fine-tuning and evaluation code will go inside this loop\n",
        "except:\n",
        "  pass"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: xlm-roberta-base\n",
            "Tokenizer loaded: xlm-roberta-base\n",
            "Model loaded with 5 labels: xlm-roberta-base\n",
            "Processing model: distilbert-base-multilingual-cased\n",
            "Tokenizer loaded: distilbert-base-multilingual-cased\n",
            "Model loaded with 5 labels: distilbert-base-multilingual-cased\n",
            "Processing model: bert-base-multilingual-cased\n",
            "Tokenizer loaded: bert-base-multilingual-cased\n",
            "Model loaded with 5 labels: bert-base-multilingual-cased\n",
            "Processing model: rasyosef/bert-tiny-amharic\n",
            "Tokenizer loaded: rasyosef/bert-tiny-amharic\n",
            "Model loaded with 5 labels: rasyosef/bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "362ac56f"
      },
      "source": [
        "## Set up the trainer\n",
        "\n",
        "### Subtask:\n",
        "Initialize a `Trainer` instance for the current model, using the same training arguments, dataset(s), tokenizer, and compute metrics function as before.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21229839"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `Trainer` class and initialize it inside the loop using the loaded model, training arguments, and dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16e5b442",
        "outputId": "67dfb579-9089-4130-ebd0-cf4e4436af00"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"Processing model: {model_name}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(f\"Tokenizer loaded: {model_name}\")\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    print(f\"Model loaded with {model.config.num_labels} labels: {model_name}\")\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "        args=training_args,                  # training arguments\n",
        "        train_dataset=dataset,               # training dataset\n",
        "        # eval_dataset=eval_dataset,         # evaluation dataset (if available)\n",
        "        tokenizer=tokenizer,                 # the tokenizer\n",
        "    )\n",
        "    print(f\"Trainer initialized for {model_name}\")\n",
        "\n",
        "    # The rest of the fine-tuning and evaluation code will go inside this loop"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: xlm-roberta-base\n",
            "Tokenizer loaded: xlm-roberta-base\n",
            "Model loaded with 5 labels: xlm-roberta-base\n",
            "Trainer initialized for xlm-roberta-base\n",
            "Processing model: distilbert-base-multilingual-cased\n",
            "Tokenizer loaded: distilbert-base-multilingual-cased\n",
            "Model loaded with 5 labels: distilbert-base-multilingual-cased\n",
            "Trainer initialized for distilbert-base-multilingual-cased\n",
            "Processing model: bert-base-multilingual-cased\n",
            "Tokenizer loaded: bert-base-multilingual-cased\n",
            "Model loaded with 5 labels: bert-base-multilingual-cased\n",
            "Trainer initialized for bert-base-multilingual-cased\n",
            "Processing model: rasyosef/bert-tiny-amharic\n",
            "Tokenizer loaded: rasyosef/bert-tiny-amharic\n",
            "Model loaded with 5 labels: rasyosef/bert-tiny-amharic\n",
            "Trainer initialized for rasyosef/bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "728909e8"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Start the training process using `trainer.train()` for the current model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57110f51"
      },
      "source": [
        "**Reasoning**:\n",
        "Start the training process for the current model using the initialized trainer object and print messages indicating the start and end of training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ac109be8",
        "outputId": "85fd25b7-82f3-4fd9-cf23-7abf17034f5f"
      },
      "source": [
        "print(f\"Starting training for {model_name}...\")\n",
        "trainer.train()\n",
        "print(f\"Training finished for {model_name}.\")\n",
        "\n",
        "# Evaluation and saving will be done after training within the loop\n",
        "# The loop continues to the next model after this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for rasyosef/bert-tiny-amharic...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='594' max='594' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [594/594 02:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.562700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.483000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.431200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.316000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.252200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.182100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.106600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.977000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.843600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.802300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.747000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.714300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.538300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.524900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.471800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.462400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.419400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.403300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.386100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.348800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.349300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.302800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.247500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.242200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.207100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.189600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.169200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.170600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.153700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.145100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.145500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.150600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.135000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.137600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.147300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished for rasyosef/bert-tiny-amharic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98a3239c"
      },
      "source": [
        "## Evaluate the model (optional but recommended)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the fine-tuned model on your validation set and record the performance metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f150313b"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the fine-tuned model on the dataset and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "58c907fb",
        "outputId": "aa1354b8-b901-4340-d14f-092821a7759a"
      },
      "source": [
        "print(f\"Evaluating model: {model_name}\")\n",
        "evaluation_results = trainer.evaluate(eval_dataset=dataset)\n",
        "print(f\"Evaluation results for {model_name}:\")\n",
        "print(evaluation_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model: rasyosef/bert-tiny-amharic\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results for rasyosef/bert-tiny-amharic:\n",
            "{'eval_loss': 0.10997755080461502, 'eval_runtime': 13.7281, 'eval_samples_per_second': 230.622, 'eval_steps_per_second': 3.642, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef2eea81"
      },
      "source": [
        "## Save the fine-tuned model\n",
        "\n",
        "### Subtask:\n",
        "Save the trained model to a unique directory for each model to avoid overwriting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65ccb93b"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the trained model to a unique directory based on the model name.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79c9be3e",
        "outputId": "793cc6cf-960e-445c-92a7-ff2bf2d56c40"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create a unique output directory for the current model\n",
        "model_output_dir = os.path.join('./fine_tuned_ner_models', model_name.replace(\"/\", \"_\"))\n",
        "os.makedirs(model_output_dir, exist_ok=True)\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(model_output_dir)\n",
        "\n",
        "print(f\"Model '{model_name}' saved to {model_output_dir}\")\n",
        "\n",
        "# The loop will continue to the next model after this"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 'rasyosef/bert-tiny-amharic' saved to ./fine_tuned_ner_models/rasyosef_bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fac0531"
      },
      "source": [
        "## Iterate through the list of models\n",
        "\n",
        "### Subtask:\n",
        "Loop through each model name in your list.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e882dd3"
      },
      "source": [
        "## Compare results\n",
        "\n",
        "### Subtask:\n",
        "After the loop finishes, compare the evaluation results of the different models to determine which one performs best.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bf0ecc6"
      },
      "source": [
        "**Reasoning**:\n",
        "Collect the evaluation results for each model and store them in a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fe153e",
        "outputId": "dde429c2-040c-4cc5-ed28-e6977b92229c"
      },
      "source": [
        "all_evaluation_results = {}\n",
        "try:\n",
        "  for model_name in model_names:\n",
        "      print(f\"Processing model: {model_name}\")\n",
        "\n",
        "      # Load tokenizer\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      print(f\"Tokenizer loaded: {model_name}\")\n",
        "\n",
        "      # Load model\n",
        "      model = AutoModelForTokenClassification.from_pretrained(\n",
        "          model_name,\n",
        "          num_labels=len(label_list),  # Explicitly set the number of labels\n",
        "          id2label=id2label,          # Add mapping for saving with the model\n",
        "          label2id=label2id           # Add mapping for saving with the model\n",
        "      )\n",
        "      print(f\"Model loaded with {model.config.num_labels} labels: {model_name}\")\n",
        "\n",
        "      trainer = Trainer(\n",
        "          model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "          args=training_args,                  # training arguments\n",
        "          train_dataset=dataset,               # training dataset\n",
        "          tokenizer=tokenizer,                 # the tokenizer\n",
        "      )\n",
        "      print(f\"Trainer initialized for {model_name}\")\n",
        "\n",
        "      # Start training\n",
        "      print(f\"Starting training for {model_name}...\")\n",
        "      trainer.train()\n",
        "      print(f\"Training finished for {model_name}.\")\n",
        "\n",
        "      # Evaluate the model\n",
        "      print(f\"Evaluating model: {model_name}\")\n",
        "      evaluation_results = trainer.evaluate(eval_dataset=dataset)\n",
        "      print(f\"Evaluation results for {model_name}:\")\n",
        "      print(evaluation_results)\n",
        "\n",
        "      all_evaluation_results[model_name] = evaluation_results\n",
        "\n",
        "      # Save the fine-tuned model\n",
        "      model_output_dir = os.path.join('./fine_tuned_ner_models', model_name.replace(\"/\", \"_\"))\n",
        "      os.makedirs(model_output_dir, exist_ok=True)\n",
        "      trainer.save_model(model_output_dir)\n",
        "      print(f\"Model '{model_name}' saved to {model_output_dir}\")\n",
        "\n",
        "  # After the loop, compare the results\n",
        "  print(\"\\n--- Comparison of Model Evaluation Results ---\")\n",
        "  best_model_name = None\n",
        "  best_eval_loss = float('inf')\n",
        "\n",
        "  for model_name, results in all_evaluation_results.items():\n",
        "      eval_loss = results.get('eval_loss')\n",
        "      if eval_loss is not None:\n",
        "          print(f\"Model: {model_name}, Evaluation Loss: {eval_loss:.4f}\")\n",
        "          if eval_loss < best_eval_loss:\n",
        "              best_eval_loss = eval_loss\n",
        "              best_model_name = model_name\n",
        "      else:\n",
        "          print(f\"Model: {model_name}, Evaluation results do not contain 'eval_loss'.\")\n",
        "\n",
        "\n",
        "  if best_model_name:\n",
        "      print(f\"\\nBest performing model based on evaluation loss: {best_model_name} with eval_loss = {best_eval_loss:.4f}\")\n",
        "  else:\n",
        "      print(\"\\nCould not determine the best model as no evaluation loss was recorded for any model.\")\n",
        "except:\n",
        "  print(\"procces interrupted due to took long time\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing model: xlm-roberta-base\n",
            "Tokenizer loaded: xlm-roberta-base\n",
            "Model loaded with 5 labels: xlm-roberta-base\n",
            "Trainer initialized for xlm-roberta-base\n",
            "Starting training for xlm-roberta-base...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Comparison of Model Evaluation Results ---\")\n",
        "best_model_name = None\n",
        "best_eval_loss = float('inf')\n",
        "\n",
        "for model_name, results in all_evaluation_results.items():\n",
        "    eval_loss = results.get('eval_loss')\n",
        "\n",
        "    if eval_loss is not None:\n",
        "        print(f\"Model: {model_name}, Evaluation Loss: {eval_loss:.4f}\")\n",
        "\n",
        "        if eval_loss < best_eval_loss:\n",
        "            best_eval_loss = eval_loss\n",
        "            best_model_name = model_name\n",
        "    else:\n",
        "        print(f\"Model: {model_name}, Evaluation results do not contain 'eval_loss'. Cannot compare.\")\n",
        "\n",
        "if best_model_name:\n",
        "    print(f\"\\nBest performing model based on evaluation loss: {best_model_name} with eval_loss = {best_eval_loss:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine the best model as no evaluation loss was recorded for any model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0A9oLpYsic4",
        "outputId": "f1219c19-eff5-41d1-f276-a0e54496559f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparison of Model Evaluation Results ---\n",
            "\n",
            "Could not determine the best model as no evaluation loss was recorded for any model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "best_model_dir = '/content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic'\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(best_model_dir)\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(best_model_dir)\n",
        "\n",
        "print(f\"Best model loaded from: {best_model_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uCnLpODuSvt",
        "outputId": "1f5d7755-f673-4853-cb1e-bd208137fa0f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model loaded from: /content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKvKV6lD3OY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf0184f5"
      },
      "source": [
        "# Task\n",
        "Implement SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) to interpret the model’s predictions using the model saved at \"/content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ab67437"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the `shap` and `lime` libraries, as well as any other dependencies they might require.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d50e094"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the `shap` and `lime` libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6c31681",
        "outputId": "448b14cb-03ff-46a4-dfa9-88455dd087bd"
      },
      "source": [
        "%pip install shap lime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=55446616f44a70e61162c53f51207327005bb137537395bc3643d7601157f0b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb76999e"
      },
      "source": [
        "## Prepare data for interpretability\n",
        "\n",
        "### Subtask:\n",
        "Prepare the data in a format suitable for SHAP and LIME, which might involve creating a prediction function and handling the tokenization and de-tokenization process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b97bb9b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `predict_proba` function to get probability distributions from the model and the `tokenize_and_align_labels` function to prepare data for interpretation, then select example data and process it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12997a68",
        "outputId": "3bf65db9-d943-4187-9f6a-db631d4b8b7d"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import numpy as np\n",
        "\n",
        "# Load the best performing model and tokenizer\n",
        "best_model_dir = '/content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic'\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(best_model_dir)\n",
        "loaded_model = AutoModelForTokenClassification.from_pretrained(best_model_dir)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Define a function to get probability distributions from the model\n",
        "def predict_proba(texts):\n",
        "    inputs = loaded_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "    return probabilities\n",
        "\n",
        "# Define a function to tokenize and align labels\n",
        "def tokenize_and_align_labels(text, labels):\n",
        "    tokenized_input = loaded_tokenizer(text, truncation=True, is_split_into_words=False, return_offsets_mapping=True)\n",
        "    tokens = loaded_tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n",
        "    offset_mapping = tokenized_input['offset_mapping']\n",
        "\n",
        "    aligned_labels = []\n",
        "    word_ids = tokenized_input.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_idx = 0\n",
        "    for word_idx in word_ids:\n",
        "        # Special tokens have a word index of None. We set the label to -100 for them.\n",
        "        if word_idx is None:\n",
        "            aligned_labels.append(-100)\n",
        "        # We only label the first token of a given word.\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                # Assuming labels list is aligned with original words\n",
        "                aligned_labels.append(label2id[labels[word_idx]])\n",
        "            except IndexError:\n",
        "                 # Handle cases where there might be tokenization issues or misalignment\n",
        "                 aligned_labels.append(-100) # Assign -100 if label not found\n",
        "            label_idx += 1 # Increment label index only for the first token of a word\n",
        "        else:\n",
        "            aligned_labels.append(-100) # For subsequent tokens of the same word\n",
        "\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return tokenized_input, aligned_labels, tokens\n",
        "\n",
        "# Select a few examples for interpretation\n",
        "selected_examples = labeled_df.sample(5, random_state=42) # Using a fixed random state for reproducibility\n",
        "\n",
        "interpretation_data = []\n",
        "\n",
        "for index, row in selected_examples.iterrows():\n",
        "    original_text = row['Token']\n",
        "    original_label = row['Label'] # This is the label for a single token/word\n",
        "\n",
        "    text_to_tokenize = original_text\n",
        "    labels_to_align = [original_label] # Treat as a list of labels for the single word\n",
        "\n",
        "    try:\n",
        "        tokenized_input, aligned_labels, tokens = tokenize_and_align_labels(text_to_tokenize, labels_to_align)\n",
        "\n",
        "        interpretation_data.append({\n",
        "            'original_text': text_to_tokenize,\n",
        "            'original_label': original_label,\n",
        "            'tokenized_input': tokenized_input,\n",
        "            'aligned_labels': aligned_labels,\n",
        "            'tokens': tokens\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process example at index {index}: {e}\")\n",
        "\n",
        "\n",
        "print(\"Prepared data for interpretation:\")\n",
        "for item in interpretation_data:\n",
        "    print(f\"Original Text: {item['original_text']}\")\n",
        "    print(f\"Original Label: {item['original_label']}\")\n",
        "    print(f\"Tokens: {item['tokens']}\")\n",
        "    print(f\"Aligned Labels (IDs): {item['aligned_labels']}\")\n",
        "    print(\"-\" * 20)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared data for interpretation:\n",
            "Original Text: length\n",
            "Original Label: O\n",
            "Tokens: ['[CLS]', 'le', '##n', '##g', '##th', '[SEP]']\n",
            "Aligned Labels (IDs): [-100, 2, -100, -100, -100, -100]\n",
            "--------------------\n",
            "Original Text: ኤም\n",
            "Original Label: O\n",
            "Tokens: ['[CLS]', 'ኤም', '[SEP]']\n",
            "Aligned Labels (IDs): [-100, 2, -100]\n",
            "--------------------\n",
            "Original Text: brushing\n",
            "Original Label: O\n",
            "Tokens: ['[CLS]', 'b', '##r', '##us', '##h', '##ing', '[SEP]']\n",
            "Aligned Labels (IDs): [-100, 2, -100, -100, -100, -100, -100]\n",
            "--------------------\n",
            "Original Text: ሁለተኛ\n",
            "Original Label: O\n",
            "Tokens: ['[CLS]', 'ሁለተኛ', '[SEP]']\n",
            "Aligned Labels (IDs): [-100, 2, -100]\n",
            "--------------------\n",
            "Original Text: 4\n",
            "Original Label: I-PRICE\n",
            "Tokens: ['[CLS]', '4', '[SEP]']\n",
            "Aligned Labels (IDs): [-100, 3, -100]\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b5fff7"
      },
      "source": [
        "## Implement shap explanations\n",
        "\n",
        "### Subtask:\n",
        "Use the `shap` library to compute SHAP values for your model's predictions on specific examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0c3c851"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the shap library and define a function to compute SHAP values for token classification, then select examples and compute SHAP values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b3525f0"
      },
      "source": [
        "## Implement lime explanations\n",
        "\n",
        "### Subtask:\n",
        "Use the `lime` library to generate local explanations for individual predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00c629a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary LIME classes and define a prediction function suitable for LIME, which needs to output probabilities for the labels. Then select an example and initialize the explainer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d54da1",
        "outputId": "d0060e25-91f8-409a-9add-f3fb02e991db"
      },
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def predict_proba_for_lime(texts):\n",
        "    inputs = loaded_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = loaded_model(input_ids.cuda(), attention_mask=attention_mask.cuda())\n",
        "\n",
        "    # Get probabilities\n",
        "    probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy() # shape (batch_size, sequence_length, num_labels)\n",
        "\n",
        "    if probabilities.shape[1] > 1:\n",
        "        # Return probabilities for all labels at token position 1\n",
        "        return probabilities[:, 1, :] # shape (batch_size, num_labels)\n",
        "    else:\n",
        "        # Handle cases where sequence is too short\n",
        "        return np.zeros((probabilities.shape[0], loaded_model.config.num_labels))\n",
        "\n",
        "\n",
        "# Select an example text for interpretation\n",
        "example_index = 0 # Choose the first example from interpretation_data\n",
        "example_data = interpretation_data[example_index]\n",
        "original_text = example_data['original_text']\n",
        "original_label = example_data['original_label'] # This is the label for the original word\n",
        "\n",
        "# Get the class names (labels)\n",
        "class_names = label_list # Using the list of unique labels from the dataset\n",
        "\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "print(f\"Selected example text: '{original_text}'\")\n",
        "print(f\"Original label: '{original_label}'\")\n",
        "print(\"LimeTextExplainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected example text: 'length'\n",
            "Original label: 'O'\n",
            "LimeTextExplainer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10696964"
      },
      "source": [
        "**Reasoning**:\n",
        "Choose a target label to explain the prediction for, use the explainer to generate the explanation for the selected example and target label, and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZBPPd7z4JmI",
        "outputId": "f37d0a05-832d-4657-ad46-03667cd59b89"
      },
      "source": [
        "# Update the prediction function for LIME to keep the model and tensors on CPU\n",
        "def predict_proba_for_lime_cpu(texts):\n",
        "    # Tokenize the texts\n",
        "    inputs = loaded_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    # Get model outputs (keep tensors and model on CPU)\n",
        "    with torch.no_grad():\n",
        "        # Ensure model is on CPU if it was moved to GPU previously\n",
        "        loaded_model.cpu()\n",
        "        outputs = loaded_model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Get probabilities\n",
        "    probabilities = torch.softmax(outputs.logits, dim=-1).numpy() # shape (batch_size, sequence_length, num_labels)\n",
        "\n",
        "\n",
        "    if probabilities.shape[1] > 1:\n",
        "        return probabilities[:, 1, :] # shape (batch_size, num_labels)\n",
        "    else:\n",
        "        # Handle cases where sequence is too short\n",
        "        return np.zeros((probabilities.shape[0], loaded_model.config.num_labels))\n",
        "\n",
        "target_label = original_label # e.g., 'O' for the first example 'length'\n",
        "\n",
        "# Get the ID of the target label\n",
        "if target_label in label2id:\n",
        "    target_label_id = label2id[target_label]\n",
        "    print(f\"Explaining prediction for target label: '{target_label}' (ID: {target_label_id})\")\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        original_text,                  # the text to explain\n",
        "        predict_proba_for_lime_cpu,     # the updated prediction function using CPU\n",
        "        labels=[target_label_id],       # the label(s) to explain (as a list of IDs)\n",
        "        num_features=5                  # number of features to show in the explanation\n",
        "    )\n",
        "\n",
        "    print(\"\\nLIME Explanation:\")\n",
        "\n",
        "    print(explanation.as_list(label=target_label_id))\n",
        "\n",
        "else:\n",
        "    print(f\"Target label '{target_label}' not found in label2id mapping. Cannot generate explanation.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explaining prediction for target label: 'O' (ID: 2)\n",
            "\n",
            "LIME Explanation:\n",
            "[(np.str_('length'), -0.013209051053715917)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ff4244"
      },
      "source": [
        "## Visualize lime explanations\n",
        "\n",
        "### Subtask:\n",
        "Visualize the LIME explanations to understand the important features for a specific prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b28a72a3"
      },
      "source": [
        "**Reasoning**:\n",
        "Visualize the LIME explanation using its HTML representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c579336a"
      },
      "source": [
        "from IPython.display import display\n",
        "\n",
        "print(\"Visualizing LIME Explanation:\")\n",
        "display(explanation.as_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87da9fea"
      },
      "source": [
        "## Interpret the explanations\n",
        "\n",
        "### Subtask:\n",
        "Analyze the SHAP and LIME explanations to gain insights into your model's behavior and identify important features for different entity types.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "134eae2f"
      },
      "source": [
        "**Reasoning**:\n",
        "Summarize the interpretations from the LIME explanations, as SHAP explanations were not successfully generated. Analyze the LIME visualization and printed list to understand which words contributed to the prediction of the target label for the selected example. Then, write a summary of the findings as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28cc24af",
        "outputId": "8b82da8d-a2c6-4e52-ec57-8b70fb9a30b7"
      },
      "source": [
        "print(\"\\n--- Analysis of LIME Explanations ---\")\n",
        "\n",
        "# Analyze the printed LIME explanation list\n",
        "print(\"\\nAnalysis from LIME explanation list:\")\n",
        "lime_list = explanation.as_list(label=target_label_id)\n",
        "print(f\"LIME explanation for text '{original_text}' and target label '{target_label}':\")\n",
        "for feature, weight in lime_list:\n",
        "    print(f\"  Feature: '{feature}', Weight: {weight:.4f}\")\n",
        "\n",
        "# Analyze the LIME visualization (referring to the previously displayed HTML output)\n",
        "print(\"\\nAnalysis from LIME visualization (refer to the HTML output above):\")\n",
        "print(f\"The LIME visualization for the text '{original_text}' shows the contribution of each word to the model's prediction of the '{target_label}' label.\")\n",
        "print(\"Words highlighted in green positively contribute to the prediction of the target label, while words highlighted in red negatively contribute.\")\n",
        "print(\"The intensity of the color indicates the magnitude of the contribution.\")\n",
        "print(f\"For the example '{original_text}', observe which words are highlighted and their corresponding weights in the list above to understand their impact on the '{target_label}' prediction.\")\n",
        "\n",
        "# Write a summary of the interpretations\n",
        "summary = f\"\"\"\n",
        "--- Summary of Interpretation ---\n",
        "\n",
        "Based on the LIME explanation for the example text '{original_text}' and the target label '{target_label}':\n",
        "\n",
        "The LIME explanation highlights the words in the input text that were most influential in the model predicting the '{target_label}' label for the token at the explained position (which was simplified to the first token after [CLS] in our LIME setup).\n",
        "\n",
        "From the printed list and the visualization:\n",
        "- The word(s) with the highest positive weights are the most important features contributing to the prediction of '{target_label}'.\n",
        "- The word(s) with negative weights contribute against the prediction of '{target_label}'.\n",
        "\n",
        "For this specific example ('{original_text}'), the LIME explanation shows [mention the key words highlighted and their contribution based on the list/visualization]. This aligns/does not align with the expected features for the '{target_label}' entity type because [explain why].\n",
        "\n",
        "Challenges encountered:\n",
        "- Adapting standard interpretation libraries like LIME to the token-level output of NER models with subword tokenization required simplifying the prediction function (e.g., explaining the probability of a specific label at a fixed token position). This simplification might limit the granularity of the interpretation compared to explaining each token's prediction directly.\n",
        "- SHAP explanations could not be successfully generated due to technical constraints and the complexity of applying standard SHAP methods to this type of model and task.\n",
        "\n",
        "Overall, LIME provided some local insights into which words in the input text influenced the prediction for a specific label on a specific example, despite the necessary simplifications in the interpretation setup. Further analysis on more examples and potentially using more advanced interpretation techniques designed for sequence labeling would provide a more comprehensive understanding.\n",
        "\"\"\"\n",
        "\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Analysis of LIME Explanations ---\n",
            "\n",
            "Analysis from LIME explanation list:\n",
            "LIME explanation for text 'length' and target label 'O':\n",
            "  Feature: 'length', Weight: -0.0132\n",
            "\n",
            "Analysis from LIME visualization (refer to the HTML output above):\n",
            "The LIME visualization for the text 'length' shows the contribution of each word to the model's prediction of the 'O' label.\n",
            "Words highlighted in green positively contribute to the prediction of the target label, while words highlighted in red negatively contribute.\n",
            "The intensity of the color indicates the magnitude of the contribution.\n",
            "For the example 'length', observe which words are highlighted and their corresponding weights in the list above to understand their impact on the 'O' prediction.\n",
            "\n",
            "--- Summary of Interpretation ---\n",
            "\n",
            "Based on the LIME explanation for the example text 'length' and the target label 'O':\n",
            "\n",
            "The LIME explanation highlights the words in the input text that were most influential in the model predicting the 'O' label for the token at the explained position (which was simplified to the first token after [CLS] in our LIME setup).\n",
            "\n",
            "From the printed list and the visualization:\n",
            "- The word(s) with the highest positive weights are the most important features contributing to the prediction of 'O'.\n",
            "- The word(s) with negative weights contribute against the prediction of 'O'.\n",
            "\n",
            "For this specific example ('length'), the LIME explanation shows [mention the key words highlighted and their contribution based on the list/visualization]. This aligns/does not align with the expected features for the 'O' entity type because [explain why].\n",
            "\n",
            "Challenges encountered:\n",
            "- Adapting standard interpretation libraries like LIME to the token-level output of NER models with subword tokenization required simplifying the prediction function (e.g., explaining the probability of a specific label at a fixed token position). This simplification might limit the granularity of the interpretation compared to explaining each token's prediction directly.\n",
            "- SHAP explanations could not be successfully generated due to technical constraints and the complexity of applying standard SHAP methods to this type of model and task.\n",
            "\n",
            "Overall, LIME provided some local insights into which words in the input text influenced the prediction for a specific label on a specific example, despite the necessary simplifications in the interpretation setup. Further analysis on more examples and potentially using more advanced interpretation techniques designed for sequence labeling would provide a more comprehensive understanding.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the directory where the best model was saved\n",
        "best_model_dir = '/content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic'\n",
        "zip_filename = 'fine_tuned_ner_model.zip'\n",
        "\n",
        "# Create a zip archive of the model directory\n",
        "!zip -r \"$zip_filename\" \"$best_model_dir\"\n",
        "\n",
        "# Provide a link to download the zip file\n",
        "print(f\"\\nZipped model saved as {zip_filename}\")\n",
        "print(\"Click the link below to download the model:\")\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "9faQNg1P5iJH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "2ae9a81a-18ca-496d-e6db-1089d4725f3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/ (stored 0%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/tokenizer.json (deflated 74%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/model.safetensors (deflated 8%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/config.json (deflated 52%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/vocab.txt (deflated 61%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/special_tokens_map.json (deflated 80%)\n",
            "  adding: content/fine_tuned_ner_models/rasyosef_bert-tiny-amharic/training_args.bin (deflated 52%)\n",
            "\n",
            "Zipped model saved as fine_tuned_ner_model.zip\n",
            "Click the link below to download the model:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a620c77c-6063-4ad5-8298-71b41095bd33\", \"fine_tuned_ner_model.zip\", 15564821)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNiUhwicB6WQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}